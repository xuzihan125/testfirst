2020.6.30 上午
完成任务1~任务7
spark可以正常启动
hadoop显示未知内部或外部命令，考虑是环境变量问题
但spark可以正常进入还未有解决方案
使用anaconda管理python环境成功安装依赖包和pyspark
但现在不能直接在命令行使用pyspark命令
考虑为计算机找不到python虚拟环境的命令
暂定解决方法为加虚拟环境的环境变量还未实现
或者在pycharm里写数据清洗代码
集群配置还有疑问留待下午解决

2020.6.30 下午
通过使用本地python环境解决了anaconda虚拟python环境和spark的使用问题
解决了Hadoop不能识别命令的问题
成功清洗了部分数据
安装了虚拟机
初步研究了ARIMA模型正在尝试python实现

2020.7.1
将Ubuntu安装至移动硬盘，并尝试搭建环境
成功配置了tomcat
开始进行前端的任务
学习html5等相关语法尝试写出简单的web并部署至tomcat
共同研究了ARIMA模型

2020.7.2
继续学习html5相关语法
了解了spring jsp serverlet等之间关系和运作原理
理清了整体数据的交互过程
理清了前端各任务之间的关系确定了接下来几天的学习和项目方向
接下来一天将继续进行相应知识储备
