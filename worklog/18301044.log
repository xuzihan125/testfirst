2020.6.30 上午
完成任务1~任务7
spark可以正常启动
hadoop显示未知内部或外部命令，考虑是环境变量问题
但spark可以正常进入还未有解决方案
使用anaconda管理python环境成功安装依赖包和pyspark
但现在不能直接在命令行使用pyspark命令
考虑为计算机找不到python虚拟环境的命令
暂定解决方法为加虚拟环境的环境变量还未实现
或者在pycharm里写数据清洗代码
集群配置还有疑问留待下午解决

2020.6.30 下午
通过使用本地python环境解决了anaconda虚拟python环境和spark的使用问题
解决了Hadoop不能识别命令的问题
成功清洗了部分数据
安装了虚拟机
初步研究了ARIMA模型正在尝试python实现

2020.7.1
将Ubuntu安装至移动硬盘，并尝试搭建环境
成功配置了tomcat
开始进行前端的任务
学习html5等相关语法尝试写出简单的web并部署至tomcat
共同研究了ARIMA模型

2020.7.2
继续学习html5相关语法
了解了spring jsp serverlet等之间关系和运作原理
理清了整体数据的交互过程
理清了前端各任务之间的关系确定了接下来几天的学习和项目方向
接下来一天将继续进行相应知识储备

2020.7.2&7.3
确定使用spring作为框架
学习spring后端相关代码学习
学习了spring框架的xml配置文件写法
其中构造器参数注入和设值注入的写法区别
以及学习了注解注入
最后学习了spring3.0以后支持的java代码当作容器的配置类
了解了bean的含义
初步认识了AOT（面向切面编程）和IOC（控制反转，也就是依赖注入DI）的原理
并尝试书写了通过容易生成bean实例的代码，算是初步运用了spring框架
然后开始学习spring框架和数据库相连的mybatis，以及和web相连的实现websocket部分遇到了阻碍
对于spring更深一层的运用尚不理解
对于写容器配置类和容器中元素实例尚且生疏
接下来的计划是继续学习spring在web和database方面的模块，并初步书写登录和接另一端传来的json类数据
然后学习flask与spring的websocket相连并努力做出迭代一
目前仍处于知识储备阶段
