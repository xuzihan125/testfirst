完成任务1~任务7
spark可以正常启动
hadoop显示未知内部或外部命令，考虑是环境变量问题
但spark可以正常进入还未有解决方案
使用anaconda管理python环境成功安装依赖包和pyspark
但现在不能直接在命令行使用pyspark命令
考虑为计算机找不到python虚拟环境的命令
暂定解决方法为加虚拟环境的环境变量还未实现
或者在pycharm里写数据清洗代码
集群配置还有疑问留待下午解决