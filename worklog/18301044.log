2020.6.30 上午
完成任务1~任务7
spark可以正常启动
hadoop显示未知内部或外部命令，考虑是环境变量问题
但spark可以正常进入还未有解决方案
使用anaconda管理python环境成功安装依赖包和pyspark
但现在不能直接在命令行使用pyspark命令
考虑为计算机找不到python虚拟环境的命令
暂定解决方法为加虚拟环境的环境变量还未实现
或者在pycharm里写数据清洗代码
集群配置还有疑问留待下午解决

2020.6.30 下午
通过使用本地python环境解决了anaconda虚拟python环境和spark的使用问题
解决了Hadoop不能识别命令的问题
成功清洗了部分数据
安装了虚拟机
初步研究了ARIMA模型正在尝试python实现
